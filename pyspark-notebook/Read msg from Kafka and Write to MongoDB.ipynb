{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, DoubleType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session and configure packages to work with kafka and mongodb\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')  # use local since we don't have a cluster\n",
    "         .appName('kafka-mongo-streaming')\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.mongodb.spark:mongo-spark-connector_2.11:2.4.0')\n",
    "         # config to be able to write data into mongo db\n",
    "         .config('spark.mongodb.output.uri', 'mongodb://root:example@mongo:27017/docstreaming.invoices?authSource=admin')\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read message from Kafka ingestion-topic\n",
    "msgs = (spark\n",
    "      .readStream\n",
    "      .format('kafka')\n",
    "      .option('kafka.bootstrap.servers', 'kafka:9092')\n",
    "      .option('subscribe', 'ingestion-topic')\n",
    "      .load()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_mongodb(msgs, epoch_id):\n",
    "    \"\"\"Transform and write data into MongoDB\n",
    "    \"\"\"\n",
    "    # cast the msg from bytes to string\n",
    "    msgs = msgs.selectExpr('CAST(key AS STRING)', 'CAST(value AS STRING)')\n",
    "    \n",
    "    # the original msg is a dataframe with col '_id' and 'value'\n",
    "    docs = msgs.withColumn(\"json_value\", F.from_json(msgs.value, MapType(StringType(), StringType())))\n",
    "    \n",
    "    # create a new df with individual columns\n",
    "    docs = (docs.select(['json_value.InvoiceNo', 'json_value.InvoiceDate', 'json_value.CustomerID', 'json_value.Country',\n",
    "                      'json_value.StockCode', 'json_value.Description', 'json_value.Quantity', 'json_value.UnitPrice']\n",
    "          ))\n",
    "    \n",
    "    # cast columns to correct data types\n",
    "    docs = (docs.withColumn('Quantity', F.col('Quantity').cast(IntegerType()))\n",
    "           .withColumn('UnitPrice', F.col('UnitPrice').cast(DoubleType()))\n",
    "           .withColumn('InvoiceDate', F.col('InvoiceDate').cast(DateType()))\n",
    "          )\n",
    "    \n",
    "    docs.write.format('com.mongodb.spark.sql.DefaultSource').mode('append').save()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs.writeStream.foreachBatch(write_data_to_mongodb).start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
